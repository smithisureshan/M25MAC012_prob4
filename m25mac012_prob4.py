# -*- coding: utf-8 -*-
"""M25MAC012_prob4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1e1NJ39aRJGVviGJ65XgtQG-eDc03KqkO

**Smithi Sureshan M25MAC012**

Importing Libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_20newsgroups
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix

"""Extracting Sports and Politics categories"""

categories=['rec.sport.baseball', 'rec.sport.hockey', 'talk.politics.guns', 'talk.politics.misc', 'talk.politics.mideast']
dataset=fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))

"""Creating Labels 0 is for sports and 1 is for politics"""

labels=[0 if i<2 else 1 for i in dataset.target]
target_columns=['Sports', 'Politics']

"""Class Distribution"""

df = pd.DataFrame({
    "text": dataset.data,
    "target": labels
})
df["category"] = df["target"].map({0: "Sports", 1: "Politics"})
print("Dataset Shape:", df.shape)
print("\nClass Distribution:")
print(df["category"].value_counts())

"""Visualization of Class Distribution"""

plt.figure(figsize=(6,4))
sns.countplot(x="category", data=df)
plt.title("Class Distribution")
plt.show()

class_counts = df["category"].value_counts()
plt.figure()
plt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%')
plt.title("Class Distribution")
plt.show()

"""Distribution of Document Length"""

df["text_length"] = df["text"].apply(lambda x: len(x.split()))
print("Statistics:")
print(df["text_length"].describe())

plt.figure(figsize=(6,4))
sns.histplot(df["text_length"], bins=50, kde=True)
plt.title("Distribution of Document Length")
plt.xlabel("Number of Words")
plt.show()

"""Frequency of words"""

from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(stop_words="english", max_features=20)
X_counts = vectorizer.fit_transform(df["text"])

sum_words = X_counts.sum(axis=0)
words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)

words = [w[0] for w in words_freq]
counts = [w[1] for w in words_freq]

plt.figure(figsize=(10,5))
sns.barplot(x=counts, y=words)
plt.title("Top 20 Most Frequent Words")
plt.xlabel("Frequency")
plt.show()

print(dataset.data[0])

Tfidf =TfidfVectorizer(stop_words='english', max_features=5000)
X=Tfidf.fit_transform(dataset.data)

feature_names = Tfidf.get_feature_names_out()
X_df = pd.DataFrame(X.toarray(), columns=feature_names)
X_df["category"] = labels

# Separate classes
sports_tfidf = X_df[X_df["category"] == 0].drop("category", axis=1)
politics_tfidf = X_df[X_df["category"] == 1].drop("category", axis=1)

# Compute mean TF-IDF score for each class
sports_mean = sports_tfidf.mean().sort_values(ascending=False)[:15]
politics_mean = politics_tfidf.mean().sort_values(ascending=False)[:15]

# Plot Sports words
plt.figure(figsize=(8,5))
sns.barplot(x=sports_mean.values, y=sports_mean.index)
plt.title("Top TF-IDF Words - Sports")
plt.xlabel("Mean TF-IDF Score")
plt.show()

# Plot Politics words
plt.figure(figsize=(8,5))
sns.barplot(x=politics_mean.values, y=politics_mean.index)
plt.title("Top TF-IDF Words - Politics")
plt.xlabel("Mean TF-IDF Score")
plt.show()

X_train, X_test, y_train, y_test=train_test_split(X, labels, test_size=0.2, random_state=42)

from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_pca = pca.fit_transform(X.toarray())

plt.figure(figsize=(8,6))
plt.scatter(X_pca[:,0], X_pca[:,1], c=labels, cmap='coolwarm', alpha=0.5)
plt.title("PCA Visualization of TF-IDF Features")
plt.xlabel("Principal Component 1")
plt.ylabel("Principal Component 2")
plt.show()

models={"Naive Bayes": MultinomialNB(), "Linear SVM": LinearSVC(dual="auto"), "Random Forest": RandomForestClassifier(n_estimators=100)}

from sklearn.metrics import classification_report

results = {}

for method, model in models.items():
    print(f"\nTraining {method}")
    model.fit(X_train, y_train)
    predict = model.predict(X_test)

    accuracy = accuracy_score(y_test, predict)

    report = classification_report(
        y_test, predict,
        target_names=target_columns,
        output_dict=True
    )

    results[method] = {
        "accuracy": accuracy,
        "precision": report["macro avg"]["precision"],
        "recall": report["macro avg"]["recall"],
        "f1": report["macro avg"]["f1-score"]
    }

    print(f"Accuracy: {accuracy:.2f}")
    print("Classification Report:")
    print(classification_report(y_test, predict, target_names=target_columns))

    matrix = confusion_matrix(y_test, predict)
    plt.figure(figsize=(5,4))
    sns.heatmap(matrix, annot=True, fmt='d',cmap='Blues',
                xticklabels=target_columns,
                yticklabels=target_columns)
    plt.title(f'Confusion Matrix: {method}')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

import numpy as np

models_names = list(results.keys())

accuracy = [results[m]["accuracy"] for m in models_names]
precision = [results[m]["precision"] for m in models_names]
recall = [results[m]["recall"] for m in models_names]
f1 = [results[m]["f1"] for m in models_names]

x = np.arange(len(models_names))
width = 0.2

plt.figure()

plt.bar(x - 1.5*width, accuracy, width, label="Accuracy")
plt.bar(x - 0.5*width, precision, width, label="Precision")
plt.bar(x + 0.5*width, recall, width, label="Recall")
plt.bar(x + 1.5*width, f1, width, label="F1-Score")

plt.xticks(x, models_names)
plt.ylabel("Score")
plt.ylim(0.85, 1.0)
plt.title("Model Performance")
plt.legend()

plt.show()